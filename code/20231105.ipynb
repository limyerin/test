{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/wlgh325/python_crawling/blob/master/1.%20%EC%B9%B4%EC%B9%B4%EC%98%A4%EB%A7%B5%20%EB%A6%AC%EB%B7%B0%20%ED%8C%8C%EC%9D%B4%EC%8D%AC%20%ED%81%AC%EB%A1%A4%EB%A7%81/kakaoMap_review_crawling.py\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################  ############\n",
    "##################### variable related selenium ##########################\n",
    "##########################################################################!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "url = \"https://map.kakao.com/\"\n",
    "\n",
    "#현재파일에 있는 크롬 드라이버를 가져와서 열기\n",
    "options = webdriver.ChromeOptions() # 크롬 브라우저 옵션\n",
    "options.add_argument('headless') # 브라우저 안 띄우기\n",
    "options.add_argument('lang=ko_KR') #  KR 언어\n",
    "chromedriver_path = \"chromedriver\" # 크롬 드라이버 위치\n",
    "#driver = webdriver.Chrome(os.path.join(os.getcwd(), chromedriver_path), options=options)  # chromedriver 열기\n",
    "driver = webdriver.Chrome(executable_path=r'C:/Users/anaco/Desktop/chromedriver-win64/chromedriver.exe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 list 만들어줌 li\n",
    "\n",
    "df=pd.read_csv('exper.csv')\n",
    "df['name']\n",
    "#len(df['name'])\n",
    "li=[]\n",
    "\n",
    "for i in range(len(df['name'])):\n",
    "    i=str(df['name'][i])+' 제주'\n",
    "    li.append(i)\n",
    "print(li)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''   내용 확인 코드------\n",
    "for i in df['name']:\n",
    "    na=str(i)\n",
    "    place_infos=[na]\n",
    "    '''\n",
    "\n",
    "for i, place in enumerate(li):\n",
    "    # delay\n",
    "    if i % 4 == 0 and i != 0:\n",
    "        sleep(5)\n",
    "    print(\"#####\", i, place)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global driver, load_wb, review_num\n",
    "    \n",
    "    driver.implicitly_wait(4)  # 렌더링 될때까지 기다린다 4초\n",
    "    driver.get('https://map.kakao.com/')  # 주소 가져오기\n",
    "\n",
    "    # 검색할 목록\n",
    "    #place_infos = ['강남 맛집']\n",
    "    \n",
    "    for i, place in enumerate(li):\n",
    "        # delay\n",
    "        if i % 4 == 0 and i != 0:\n",
    "            sleep(1)\n",
    "        print(\"#####\", i)\n",
    "        search(place)\n",
    "      \n",
    "    driver.quit()\n",
    "    print(\"finish\")\n",
    "\n",
    "\n",
    "def search(place):\n",
    "    global driver\n",
    "\n",
    "    search_area = driver.find_element(By.XPATH,'//*[@id=\"search.keyword.query\"]')  # 검색 창\n",
    "    search_area.send_keys(place)  # 검색어 입력\n",
    "    driver.find_element(By.XPATH,'//*[@id=\"search.keyword.submit\"]').send_keys(Keys.ENTER)  # Enter로 검색\n",
    "    sleep(0.5)\n",
    "\n",
    "    # 검색된 정보가 있는 경우에만 탐색\n",
    "    # 1번 페이지 place list 읽기\n",
    "    html = driver.page_source\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    place_lists = soup.select('.placelist > .PlaceItem') # 검색된 장소 목록\n",
    "\n",
    "    # 검색된 첫 페이지 장소 목록 크롤링하기\n",
    "    crawling(place, place_lists)########################### re부터 return까지 수정\n",
    "    search_area.clear()\n",
    "\n",
    "    # 우선 더보기 클릭해서 2페이지\n",
    "    try:\n",
    "        driver.find_element(By.XPATH,'//*[@id=\"info.search.place.more\"]').send_keys(Keys.ENTER)\n",
    "        sleep(0.5)\n",
    "\n",
    "        # 2~ 5페이지 읽기 ~3\n",
    "        for i in range(2, 5):\n",
    "            # 페이지 넘기기\n",
    "            xPath = '//*[@id=\"info.search.page.no' + str(i) + '\"]'\n",
    "            driver.find_element(By.XPATH,xPath).send_keys(Keys.ENTER)\n",
    "            sleep(1)\n",
    "\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            place_lists = soup.select('.placelist > .PlaceItem') # 장소 목록 list\n",
    "\n",
    "            crawling(place, place_lists)\n",
    "\n",
    "    except ElementNotInteractableException:\n",
    "        print('not found')\n",
    "    finally:\n",
    "        search_area.clear()\n",
    "        \n",
    "\n",
    "\n",
    "def crawling(place, place_lists):\n",
    "    \"\"\"\n",
    "    페이지 목록을 받아서 크롤링 하는 함수\n",
    "    :param place: 리뷰 정보 찾을 장소이름\n",
    "    \"\"\"\n",
    "\n",
    "    while_flag = False\n",
    "    for i, place in enumerate(place_lists):\n",
    "        # 광고에 따라서 index 조정해야함\n",
    "        #if i >= 3:\n",
    "         #   i += 1\n",
    "\n",
    "        place_name = place.select('.head_item > .tit_name > .link_name')[0].text  # place name\n",
    "        place_address = place.select('.info_item > .addr > p')[0].text  # place address\n",
    "        \n",
    "        detail_page_xpath = '//*[@id=\"info.search.place.list\"]/li[' + str(i + 1) + ']/div[5]/div[4]/a[1]'\n",
    "        driver.find_element(By.XPATH, detail_page_xpath).send_keys(Keys.ENTER)\n",
    "        driver.switch_to.window(driver.window_handles[-1])  # 상세정보 탭으로 변환\n",
    "        sleep(1)\n",
    "\n",
    "        print('####', place_name)\n",
    "\n",
    "        # 첫 페이지\n",
    "        extract_review(place_name)\n",
    "        #driver.find_element(By.XPATH, '//*[@id=\"mArticle\"]/div[8]/div[3]/a/span[1]').send_keys(Keys.ENTER)\n",
    "        \n",
    "\n",
    "        # 2-5 페이지\n",
    "        idx = 3\n",
    "        try:\n",
    "            page_num = len(driver.find_elements(By.CLASS_NAME,'link_page')) # 페이지 수 찾기\n",
    "            for i in range(page_num-1):\n",
    "                # css selector를 이용해 페이지 버튼 누르기\n",
    "                #driver.find_element(By.CSS_SELECTOR,'#mArticle > div.cont_evaluation > div.evaluation_review > div > a:nth-child(' + str(idx) +')').send_keys(Keys.ENTER)\n",
    "                driver.find_element(By.XPATH, '//*[@id=\"mArticle\"]/div[8]/div[3]/a/span[1]').send_keys(Keys.ENTER)\n",
    "                sleep(1)\n",
    "                extract_review(place_name)\n",
    "                idx += 1\n",
    "            driver.find_element(By.LINK_TEXT,'다음').send_keys(Keys.ENTER) # 5페이지가 넘는 경우 다음 버튼 누르기\n",
    "            sleep(1)\n",
    "            extract_review(place_name) # 리뷰 추출\n",
    "        except (NoSuchElementException, ElementNotInteractableException):\n",
    "            print(\"no review in crawling\")\n",
    "            \n",
    "        # 그 이후 페이지\n",
    "        while True:\n",
    "            idx = 4\n",
    "            try:\n",
    "                page_num = len(driver.find_elements(By.CLASS_NAME,'link_page'))\n",
    "                for i in range(page_num-1):\n",
    "                    driver.find_element(By.CSS_SELECTOR,'#mArticle > div.cont_evaluation > div.evaluation_review > div > a:nth-child(' + str(idx) +')').send_keys(Keys.ENTER)\n",
    "                    sleep(1)\n",
    "                    extract_review(place_name)\n",
    "                    idx += 1\n",
    "                driver.find_element(By.LINK_TEXT,'다음').send_keys(Keys.ENTER) # 10페이지 이상으로 넘어가기 위한 다음 버튼 클릭\n",
    "                sleep(1)\n",
    "                extract_review(place_name) # 리뷰 추출\n",
    "            except (NoSuchElementException, ElementNotInteractableException):\n",
    "                print(\"no review in crawling\")\n",
    "                break\n",
    "\n",
    "        driver.close()\n",
    "        driver.switch_to.window(driver.window_handles[0])  # 검색 탭으로 전환\n",
    "\n",
    "\n",
    "# 실질적으로 리뷰 모이는 부분\n",
    "def extract_review(place_name):\n",
    "    global driver\n",
    "\n",
    "    ret = True\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # 첫 페이지 리뷰 목록 찾기\n",
    "    review_lists = soup.select('.list_evaluation > li')\n",
    "   \n",
    "\n",
    "    # 리뷰가 있는 경우\n",
    "    if len(review_lists) != 0:\n",
    "        for i, review in enumerate(review_lists):\n",
    "            \n",
    "            comment = review.select('.txt_comment > span') # 리뷰\n",
    "            rating = review.select('.grade_star > em') # 별점\n",
    "            #comment = review.select('.list_evaluation > li> comment_info > txt_comment > span') # 리뷰############수정\n",
    "            #rating = review.select('.list_evaluation > li > unit_info > txt_desc') # 별점############수정\n",
    "            val = ''\n",
    "            if len(comment) != 0:\n",
    "                if len(rating) != 0:\n",
    "                    val = comment[0].text + '/' + rating[0].text.replace('점', '')\n",
    "                \n",
    "                else:\n",
    "                    val = comment[0].text + '/0'\n",
    "                print(val)\n",
    "\n",
    "    else:\n",
    "        print('no review in extract')\n",
    "        ret = False\n",
    "\n",
    "    return ret   \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
